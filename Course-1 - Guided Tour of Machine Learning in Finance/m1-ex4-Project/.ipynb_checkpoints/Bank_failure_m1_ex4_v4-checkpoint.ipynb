{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling of bank failures by FDIC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import os\n",
    "import functools\n",
    "import math\n",
    "import random\n",
    "import sys, getopt\n",
    "import sklearn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "except:\n",
    "    pass\n",
    "print('scikit-learn version:', sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ONLY FOR GRADING. DO NOT EDIT ###\n",
    "submissions=dict()\n",
    "assignment_key=\"rJ8y4n45EeiHwwrEIRIXdA\" \n",
    "all_parts=[\"o5YYT\", \"2cHUA\", \"5br8u\",\"Mxrav\", \"JFNf3\"]\n",
    "### ONLY FOR GRADING. DO NOT EDIT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = # the key provided to the Student under his/her email on submission page\n",
    "COURSERA_EMAIL = # the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common cell - share this across notebooks\n",
    "state_cols = ['log_TA','NI_to_TA', 'Equity_to_TA', 'NPL_to_TL', 'REO_to_TA', \n",
    "              'ALLL_to_TL', 'core_deposits_to_TA', 'brokered_deposits_to_TA', \n",
    "              'liquid_assets_to_TA', 'loss_provision_to_TL', 'NIM', 'assets_growth']\n",
    "\n",
    "all_MEVs = np.array(['term_spread',\n",
    "                    'stock_mkt_growth',\n",
    "                    'real_gdp_growth',\n",
    "                    'unemployment_rate_change',\n",
    "                    'treasury_yield_3m',\n",
    "                    'bbb_spread',\n",
    "                    'bbb_spread_change'])\n",
    "\n",
    "MEV_cols = all_MEVs.tolist()\n",
    "\n",
    "next_state_cols = ['log_TA_plus_1Q','NI_to_TA_plus_1Q', 'Equity_to_TA_plus_1Q', 'NPL_to_TL_plus_1Q', 'REO_to_TA_plus_1Q', \n",
    "                   'ALLL_to_TL_plus_1Q', 'core_deposits_to_TA_plus_1Q', 'brokered_deposits_to_TA_plus_1Q', \n",
    "                   'liquid_assets_to_TA_plus_1Q', 'loss_provision_to_TL_plus_1Q', \n",
    "                   'ROA_plus_1Q', \n",
    "                   'NIM_plus_1Q', \n",
    "                   'assets_growth_plus_1Q', \n",
    "                   'FDIC_assessment_base_plus_1Q_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf('../readonly/df_train_FDIC_defaults_1Y.h5', key='df')\n",
    "df_test = pd.read_hdf('../readonly/df_test_FDIC_defaults_1Y.h5', key='df')\n",
    "df_data = pd.read_hdf('../readonly/data_adj_FDIC_small.h5', key='df')\n",
    "df_closure_learn = pd.read_hdf('../readonly/df_FDIC_learn.h5',key='df')\n",
    "print(df_closure_learn.index.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct training and testing datasets for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.plot(x=state_cols[0], y='defaulter', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot 4 scatter plots together\n",
    "\n",
    "# log_TA / NI_to_TA\n",
    "# log_TA / NPL_to_TL\n",
    "# log_TA / Equity_to_TA\n",
    "# log_TA /ROA\n",
    "\n",
    "first_indx = [0, 0, 0, 0]\n",
    "second_indx = [1, 3, 2, 10]\n",
    "\n",
    "X_train = df_train[state_cols].values\n",
    "y_train = df_train.defaulter.values # .reshape(-1,1)\n",
    "\n",
    "num_plots = 4\n",
    "if num_plots % 2 == 0:\n",
    "    f, axs = plt.subplots(num_plots // 2, 2)\n",
    "else:\n",
    "    f, axs = plt.subplots(num_plots// 2 + 1, 2)\n",
    "    \n",
    "f.subplots_adjust(hspace=.3)\n",
    "\n",
    "f.set_figheight(10.0)\n",
    "f.set_figwidth(10.0)\n",
    "    \n",
    "for i in range(num_plots):\n",
    "    if i % 2 == 0:\n",
    "        first_idx = i // 2\n",
    "        second_idx = 0\n",
    "    else:\n",
    "        first_idx = i // 2\n",
    "        second_idx = 1\n",
    "        \n",
    "    axs[first_idx,second_idx].plot(X_train[y_train == 1.0, first_indx[i]], \n",
    "                                   X_train[y_train == 1.0, second_indx[i]], 'r^', label=\"Failed\")\n",
    "    axs[first_idx,second_idx].plot(X_train[y_train == 0.0, first_indx[i]], \n",
    "                                   X_train[y_train == 0.0, second_indx[i]], 'go',label=\"Non-failed\") \n",
    "    \n",
    "    axs[first_idx, second_idx].legend()\n",
    "    axs[first_idx, second_idx].set_xlabel('%s' % state_cols[first_indx[i]])\n",
    "    axs[first_idx, second_idx].set_ylabel('%s' % state_cols[second_indx[i]])\n",
    "    axs[first_idx, second_idx].set_title('Failed banks vs non-failed banks')\n",
    "    axs[first_idx, second_idx].grid(True)\n",
    "    \n",
    "if num_plots % 2 != 0:\n",
    "    f.delaxes(axs[i // 2, 1])\n",
    "    \n",
    "# plt.savefig('Failed_vs_nonfailed_rr_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(model, df_test, y_true, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    model - trained model such as DecisionTreeClassifier, etc.\n",
    "    df_test - Data Frame of predictors\n",
    "    y_true - True binary labels in range {0, 1} or {-1, 1}. If labels are not binary, pos_label should be explicitly given.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        return 0., 0., 0.\n",
    "    \n",
    "    # prediction \n",
    "    predicted_sm = model.predict(df_test, linear=False)\n",
    "    predicted_binary = (predicted_sm > threshold).astype(int)\n",
    "\n",
    "    # print(predicted_sm.shape, y_true.shape)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, predicted_sm, pos_label=1)\n",
    "    \n",
    "    # compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    ks = np.max(tpr - fpr) # Kolmogorov - Smirnov test\n",
    "\n",
    "    # note that here teY[:,0] is the same as df_test.default_within_1Y\n",
    "    accuracy_score = metrics.accuracy_score(y_true, predicted_binary)\n",
    "    \n",
    "    # equivalently, Area Under the ROC Curve could be computed as:\n",
    "    # compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "    # auc_score = metrics.roc_auc_score(y_true, predicted_sm)\n",
    "\n",
    "    try:\n",
    "        plt.title('Logistic Regression ROC curve')\n",
    "        plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.plot([0,1], [0,1], 'r--')\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "\n",
    "        # plt.savefig('ROC_curve_1.png')\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return roc_auc, accuracy_score, ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_train(df_train, df_test, choice=0, predict_within_1Y=False):\n",
    "    \"\"\"\n",
    "    make the train and test datasets\n",
    "    Arguments:\n",
    "    choice - an integer 0 or -1. Controls selection of predictors. \n",
    "    Add tangible equity and assessment base as predictors\n",
    "\n",
    "    predict_within_1Y - boolean  if True, predict defaults within one year\n",
    "    Return:\n",
    "        a tuple of:\n",
    "        - training data set predictors, np.array\n",
    "        - training data set : variable to predict, np.array\n",
    "        - test data set : variable to predict, np.array\n",
    "        - predictor variable names\n",
    "    \"\"\"\n",
    "    \n",
    "    if choice == -1: # only state cols\n",
    "        predictors = state_cols\n",
    "    elif choice == 0:  # original variables\n",
    "        predictors = state_cols + MEV_cols \n",
    "\n",
    "    trX = df_train[predictors].values\n",
    "    teX = df_test[predictors].values\n",
    "    num_features = len(predictors)    \n",
    "    num_classes = 2\n",
    "\n",
    "    if predict_within_1Y == True:\n",
    "        trY = df_train[['default_within_1Y','no_default_within_1Y']].values\n",
    "        teY = df_test[['default_within_1Y','no_default_within_1Y']].values\n",
    "    else:\n",
    "        trY = df_train[['defaulter','non_defaulter']].values\n",
    "        teY = df_test[['defaulter','non_defaulter']].values\n",
    "    return trX, trY, teX, teY, predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at correlations\n",
    "df_train[MEV_cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with statsmodels\n",
    "\n",
    "### Part 1\n",
    "Perform logistic regression using **cols_to_use** as predictors. Use df_train pandas DataFrame as training data set, and df_test pandas DataDrame as testing data set to perform prediction based on the already trained model. Utilize statsmodels package. The result of fitting logistic regression should be assigned to variable named **model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "\n",
    "cols_to_use = state_cols + MEV_cols  + ['const']\n",
    "model = None\n",
    "df_train['const'] = 1\n",
    "\n",
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "# ....\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction \n",
    "predicted_sm = np.array([])\n",
    "\n",
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_binary = (predicted_sm > threshold).astype(int)\n",
    "auc_score, accuracy_score, ks = calc_metrics(model, df_test[cols_to_use], df_test.defaulter)\n",
    "\n",
    "print('Accuracy score %f' % accuracy_score)\n",
    "print('AUC score %f' % auc_score)\n",
    "print('Kolmogorov-Smirnov statistic %f' % ks)\n",
    "\n",
    "# note that here teY[:,0] is the same as df_test.default_within_1Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_1=[accuracy_score, auc_score, ks]\n",
    "\n",
    "try:\n",
    "    part1 = \" \".join(map(repr, part_1))\n",
    "except TypeError:\n",
    "    part1 = repr(part_1)    \n",
    "    \n",
    "submissions[all_parts[0]]=part1\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[0],all_parts,submissions)\n",
    "[accuracy_score, auc_score, ks]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with sklearn\n",
    "\n",
    "### Part 2 \n",
    "In Part 2 you will use scikit-learn to perform logistic regression using the same training and test datasets.\n",
    "Once the model is trained using trX, thisTrY, test it using teX, thisTeY and compute logistic regression score.\n",
    "\n",
    "- Use **\"l1\"** penalty\n",
    "- Set inverse of regularization strength to **1000.0**; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "- Set tolerance to **1e-6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, linear_model\n",
    "\n",
    "trX, trY, teX, teY, predictors = make_test_train(df_train, df_test)\n",
    "lr_score = 0.\n",
    "thisTrY = trY[:,0]\n",
    "thisTeY = teY[:,0]\n",
    "\n",
    "logistic = None # instantiate a model and reference it\n",
    "result = None # result of fitting the model\n",
    "\n",
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "# .... define random_state argment in logistic regression class. Ininitialize it to 42\n",
    "# such as this: random_state=42\n",
    "# the variable name required for grading lr_score\n",
    "\n",
    "### END CODE HERE ###\n",
    "print('LogisticRegression score: %f' % lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part2=str(lr_score)   \n",
    "submissions[all_parts[1]]=part2\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:2],all_parts,submissions)\n",
    "lr_score\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "In this part you will again use scikit learn logistic regression but with different set of predictors. This will be a smaller set of predictor variables based on the analysis of P-values from the logistic regression. Use cols_to_use as predictors in df_train and df_test data sets. Use  **defaulter** column as something to predict.\n",
    "\n",
    "Initialize reference to the logistic regression model **logistic** with an instance of appropriate class from  scikit learn module and let **result** be the result of fitting the model to the training data set.\n",
    "\n",
    "Just as before initialize the model with the following parameters:\n",
    "- Use **\"l1\"** penalty\n",
    "- Set inverse of regularization strength to **1000.0**; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "- Set tolerance to **1e-6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do Logistic Regression with a smaller number of predictor, based on analysis of P-values \n",
    "# for the logistic regression with a full set of variables\n",
    "\n",
    "# a smaller set is based on the analysis of P-values for the logistic regression\n",
    "cols_to_use = ['log_TA', 'NI_to_TA', 'Equity_to_TA', 'NPL_to_TL',\n",
    "               'core_deposits_to_TA',\n",
    "               'brokered_deposits_to_TA',\n",
    "               'liquid_assets_to_TA'\n",
    "              ] + ['term_spread', 'stock_mkt_growth']\n",
    "\n",
    "lr_score = 0.\n",
    "logistic = None\n",
    "result = None\n",
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "# .... when initializing logistic regression class in 'sklearn', set random_state to 42 like this: random_state=42\n",
    "# ... like this: random_state=42\n",
    "# ... for grading, please store the logistic regression model into variable : logistic\n",
    "\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# combine results of the Logistic Regression to a small dataframe df_coeffs_LR\n",
    "df_coeffs_LR = pd.DataFrame({0: np.array([0.] * (len(cols_to_use) + 1), dtype=np.float32)})\n",
    "if logistic is not None:\n",
    "    model_params = np.hstack((logistic.coef_[0], logistic.intercept_))\n",
    "    df_coeffs_LR = pd.DataFrame(data=model_params, index=cols_to_use + ['const'])\n",
    "    df_coeffs_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_3=list(df_coeffs_LR.values.squeeze())\n",
    "try:\n",
    "    part3 = \" \".join(map(repr, part_3))\n",
    "except TypeError:\n",
    "    part3 = repr(part_3)    \n",
    "submissions[all_parts[2]]=part3\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:3],all_parts,submissions)\n",
    "df_coeffs_LR.values.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup inputs and expeced outputs for Logistic Regression using Tensorflow\n",
    "cols = state_cols + MEV_cols\n",
    "# inputs to Logistic Regression (via Tensorflow)\n",
    "X_trainTf = df_train[cols].values\n",
    "X_testTf = df_test[cols].values\n",
    "\n",
    "# add constant columns to both\n",
    "X_trainTf = np.hstack((np.ones((X_trainTf.shape[0], 1)), X_trainTf))\n",
    "X_testTf = np.hstack((np.ones((X_testTf.shape[0], 1)), X_testTf))\n",
    "\n",
    "# exepectd outputs:\n",
    "y_trainTf = df_train.defaulter.astype('int').values.reshape(-1,1)\n",
    "y_testTf = df_test.defaulter.astype('int').values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Unique values to predict:', np.unique(y_trainTf))\n",
    "print('Number of samples to train on:', y_trainTf.shape[0])\n",
    "print('Number of samples to test on:', y_testTf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    np.random.seed(42)\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Logistic Regression TF model\n",
    "\n",
    "**instructions**\n",
    "\n",
    "in tensorflow create: \n",
    " - placeholder for inputs called 'X' \n",
    " - placeholder for inputs called 'y'\n",
    " - variable for model parameters called 'theta', initialized with theta_init\n",
    "\n",
    "loss function: use log loss\n",
    "optimizer: use Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define the model\n",
    "reset_graph()\n",
    "n_inputs = X_trainTf.shape[1]\n",
    "learning_rate = 0.01\n",
    "theta_init = tf.random_uniform([n_inputs, 1], -1.0, 1.0, seed=42)\n",
    "\n",
    "# build Logistic Regression model using Tensorflow\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(theta_init, name=\"theta\")\n",
    "\n",
    "### START CODE HERE ### (≈ 6-7 lines of code)\n",
    "### ....\n",
    "### .... for grading please store probabilities in y_proba\n",
    "\n",
    "y_proba =                  # = 1 / (1 + tf.exp(-logits))\n",
    "\n",
    "# uses epsilon = 1e-7 by default to regularize the log function\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train Logistic Regression TF model\n",
    "\n",
    "**Instructions**\n",
    "- Use random_batch() function to grab batches from X_trainTf and y_trainTf.\n",
    "- Once the model is trained evaluate it based on X_testTf and y_testTf. \n",
    "- The **y_proba_val** should be assigned the result of the evaluation on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1001\n",
    "batch_size = 50\n",
    "num_rec = X_trainTf.shape[0]\n",
    "n_batches = int(np.ceil(num_rec / batch_size))\n",
    "\n",
    "y_proba_val = np.array([], dtype=np.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    ### START CODE HERE ### (≈ 6-7 lines of code)\n",
    "    ## ....\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "    \n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "threshold = 0.5\n",
    "y_pred = (y_proba_val >= threshold)\n",
    "print(np.sum(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate precision, recall, and AUC\n",
    "\n",
    "auc_score = 0.\n",
    "ks = 0.\n",
    "roc_auc = 0.\n",
    "recall = 0.\n",
    "precision = 0.\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "if y_proba_val.shape == y_testTf.shape:\n",
    "    precision = precision_score(y_testTf, y_pred)\n",
    "    recall = recall_score(y_testTf, y_pred)\n",
    "    auc_score = metrics.roc_auc_score(y_testTf, y_proba_val)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_testTf, y_proba_val, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    ks = np.max(tpr - fpr)\n",
    "\n",
    "    print('precision: ', precision)\n",
    "    print('recall: ', recall)\n",
    "    print('AUC score = ', auc_score)\n",
    "    print('roc_auc = ', roc_auc)\n",
    "    print('KS_test = ', ks)\n",
    "\n",
    "    try:\n",
    "        plt.title('ROC_curve')\n",
    "        plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.plot([0,1], [0,1], 'r--')\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.savefig('ROC_curve_TF.png')\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_4=list([precision, recall, roc_auc, ks])\n",
    "try:\n",
    "    part4 = \" \".join(map(repr, part_4))\n",
    "except TypeError:\n",
    "    part4 = repr(part_4)\n",
    "submissions[all_parts[3]]=part4\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:4],all_parts,submissions)\n",
    "[precision, recall, roc_auc, ks]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = state_cols + MEV_cols\n",
    "n_inputs = len(cols)\n",
    "\n",
    "# inputs \n",
    "X_trainTf = df_train[cols].values\n",
    "X_testTf = df_test[cols].values\n",
    "\n",
    "# outputs \n",
    "y_trainTf = df_train['defaulter'].astype('int').values.reshape(-1,)\n",
    "y_testTf = df_test['defaulter'].astype('int').values.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        tf.set_random_seed(42)\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Neural Network\n",
    "\n",
    "**Instructions**\n",
    "Implement Neural Network with two hidden layers. The number of nodes in the first and the second hidden layers is **n_hidden1** and **n_hidden2** correspondingly.\n",
    "Use neuron_layer() function to construct neural network layers.\n",
    "\n",
    "- Use ReLU activation function for hidden layers\n",
    "- The output layer has **n_outputs** and does not have an activation function\n",
    "- Use sparse softmax cross-entropy with logits as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden1 = 20\n",
    "n_hidden2 = 10\n",
    "n_outputs = 2 # binary classification (defaulted, not defaulted bank)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "### START CODE HERE ### (≈ 10-15 lines of code)\n",
    "### ...\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network\n",
    "\n",
    "**Instructions**\n",
    "Train neural network passing batches of inputs of size **batch_size**, which predicts bank defaults / non-defaults. Once the network is trained, evaluate accuracy using **X_testTf**, **y_testTf**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "n_epochs = 400\n",
    "batch_size = 50\n",
    "num_rec = X_trainTf.shape[0]\n",
    "n_batches = int(np.ceil(num_rec / batch_size))\n",
    "acc_test = 0. #  assign the result of accuracy testing to this variable\n",
    "\n",
    "### START CODE HERE ### (≈ 9-10 lines of code)\n",
    "# ... variable required for testing acc_test\n",
    "with tf.Session() as sess:\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part5=str(acc_test)\n",
    "submissions[all_parts[4]]=part5\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:5],all_parts,submissions)\n",
    "acc_test\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "copy-of-guided-tour-machine-learning-finance"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
