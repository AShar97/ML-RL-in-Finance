{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tobit regression with TensorFlow\n",
    "\n",
    "Tobit regression fits the following model for non-negative data $ y $: \n",
    "\n",
    "$  y({\\bf X}) = \\max (0, w_0 + \\sum_{i=1}^{N} w_i X_i + w_{N+1} \\cdot \\varepsilon) $  \n",
    "\n",
    "Here $ X_i $ are predictors, $ \\varepsilon \\sim N(0,1) $ is a standard Gaussian noise, and $ w_{N+1} $ is the noise\n",
    "volatility (standard deviation).\n",
    "\n",
    "Our problem is to fit parameters $ N+2 $ parameters $ w_{i} $ for $ i = 0, \\ldots, N+1 $ to the observed set of pairs $ \\left({\\bf X}_i, y_i \\right) $  \n",
    "\n",
    "We use synthetic data with known parameters to learn how to implement Tobit Regression in TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook. \n",
    "\n",
    "We will often specify \"(≈ X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
    "\n",
    "- The blue button \"Submit Assignment\" does not work. After running all the cells, please go directly to Assignment-> My submission to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ONLY FOR GRADING. DO NOT EDIT ###\n",
    "submissions=dict()\n",
    "assignment_key=\"w3Hc-vZdEeehlBIKDnZryg\" \n",
    "all_parts=[\"pLnY5\", \"RKR6p\", \"IU1pw\", \"ISVtY\", \"Cutr3\"]\n",
    "### ONLY FOR GRADING. DO NOT EDIT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COURSERA_TOKEN = # the key provided to the Student under his/her email on submission page\n",
    "COURSERA_EMAIL = # the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function  to reset the TF graph to the same state each time\n",
    "def reset_graph(seed=42):\n",
    "    # to make results reproducible across runs\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tobit Regression class\n",
    "\n",
    "**Instructions**:\n",
    "Complete the code for the calculation of loss function (the negative log-likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tobit_Regression:\n",
    "    \n",
    "    def __init__(self, n_features, learning_rate=0.005, L=0):\n",
    "        \n",
    "        self.input = tf.placeholder(tf.float32, [None, n_features], name=\"Input\")\n",
    "        self.target = tf.placeholder(tf.float32, [None, 1], name=\"Target\")\n",
    "        \n",
    "        # the first weight is for the intercept, the last one is for a square root of the noise std \n",
    "        self.weights = tf.Variable(tf.random_normal([n_features + 2, 1]))\n",
    "        \n",
    "        # Augmented data matrix is obtained by adding a column of ones to the data matrix\n",
    "        self.data_plus_bias = tf.concat([tf.ones([tf.shape(self.input)[0], 1]), self.input], axis=1)\n",
    "\n",
    "        #######################################################################\n",
    "        # MLE for Tobit regression \n",
    "        \n",
    "        # noise volatility is obtained as a square of the last weight to ensure positivity \n",
    "        self.sigma = 0.0001 + tf.square(self.weights[-1])\n",
    "        \n",
    "        # term1 and term2 are just placeholders initialized such that the code runs\n",
    "        # students need to initialize them appropriately to solve this assignment\n",
    "        term1 = tf.Variable(np.zeros(shape=(n_features + 2, 1)))\n",
    "        term2 = tf.Variable(np.zeros(shape=(n_features + 2, 1)))\n",
    "        # THIS IS THE PART THAT STUDENTS ARE SUPPOSED TO WRITE THEMSELVES TO COMPLETE THE IMPLEMENTATION \n",
    "        # OF THE TOBIT REGRESSION MODEL\n",
    "        \n",
    "        # FOR THE ASSIGNMENT: complete the code for the calculation of loss function \n",
    "        # (the negative log-likelihood)\n",
    "        ### START CODE HERE ### (≈ 6-7 lines of code)\n",
    "\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        self.loss = - tf.reduce_mean(term1 + term2)\n",
    "        \n",
    "        #####################################################################\n",
    "\n",
    "        # Use Adam optimization for training\n",
    "        self.train_step = (tf.train.AdamOptimizer(learning_rate).minimize(self.loss), -self.loss)\n",
    "        \n",
    "        # prediction made from the model: Use a ReLU neuron!\n",
    "        self.output = tf.nn.relu(tf.matmul(self.data_plus_bias[:, :], self.weights[:-1]))\n",
    "        \n",
    "        # Check the output L1-norm error  \n",
    "        self.output_L1_error = tf.reduce_mean(tf.abs(self.target - self.output))\n",
    "\n",
    "    def generate_data(n_points,\n",
    "                      n_features,\n",
    "                      weights,\n",
    "                      noise_std):\n",
    "\n",
    "        # Bounds of [-1,1] in space of n_points x n_features\n",
    "        np.random.seed(42)\n",
    "        bias = np.ones(n_points).reshape((-1,1))\n",
    "        low = - np.ones((n_points,n_features),'float')\n",
    "        high = np.ones((n_points,n_features),'float')\n",
    "\n",
    "        # simulated features are uniformally distributed on [-1,1].\n",
    "        # The size n_points x n_features of array X is inferred by broadcasting of 'low' and 'high'\n",
    "        X = np.random.uniform(low=low, high=high)\n",
    "        \n",
    "        # simulated noise\n",
    "        noise = np.random.normal(size=(n_points, 1))\n",
    "        \n",
    "        # outputs    \n",
    "        Y = weights[0] * bias + np.dot(X, weights[1:]).reshape((-1,1)) + noise_std * noise\n",
    "\n",
    "        # truncate negative values of Y    \n",
    "        np.clip(Y, a_min=0, a_max=None, out=Y)\n",
    "\n",
    "        return X, Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_tobit_dataset(n_points, n_features, train_test_split=4):\n",
    "    \"\"\"\n",
    "    Generate dataset for Tobit regression model and split it into training and test portions\n",
    "    \n",
    "    \"\"\"\n",
    "    # n_features + 1 weights (one for a constant feature)\n",
    "    data_weights = np.array([-0.25, 0.5, 0.2, .1]) \n",
    "    noise_std = 0.1\n",
    "    \n",
    "    # Generate dataset\n",
    "    X, Y = Tobit_Regression.generate_data(n_points=n_points,\n",
    "                                           n_features=n_features,\n",
    "                                           weights=data_weights,\n",
    "                                           noise_std=noise_std)\n",
    "    \n",
    "    # split to the train and test set\n",
    "    # 1/4 of the data is used for a test\n",
    "    \n",
    "    n_test = int(n_points / train_test_split)\n",
    "    n_train = n_points - n_test\n",
    "    \n",
    "    X_train = X[:n_train,:]\n",
    "    Y_train = Y[:n_train].reshape((-1,1))\n",
    "\n",
    "    X_test = X[n_train:,:]\n",
    "    Y_test = Y[n_train:].reshape((-1,1))\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def train_model(n_features, learning_rate, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Train Tobit Regression model\n",
    "    \n",
    "    Return:\n",
    "        a tuple of:\n",
    "        - Model fitted weights, np.array\n",
    "        - loss, double \n",
    "        - fitted noise std error, double\n",
    "        - L1 error, double\n",
    "    \"\"\"\n",
    "    # create an instance of the Tobit Regression class  \n",
    "    model = Tobit_Regression(n_features=n_features, learning_rate=learning_rate)\n",
    "\n",
    "    # train the model\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for _ in range(0, n_steps):\n",
    "            (_, loss), weights = sess.run((model.train_step, model.weights), feed_dict={\n",
    "                model.input: X_train,\n",
    "                model.target: Y_train\n",
    "                })\n",
    "    \n",
    "        # predictions for the test set\n",
    "        # std_model = weights[-1]**2     \n",
    "        output, std_model = sess.run([model.output,model.sigma], \n",
    "                                     feed_dict={model.input: X_test})\n",
    "        \n",
    "        output_L1_error = sess.run(model.output_L1_error,\n",
    "                                   feed_dict={model.input: X_test,\n",
    "                                   model.target: Y_test})\n",
    "        sess.close()\n",
    "    return weights[:-1], loss, std_model[0], output_L1_error, output\n",
    "\n",
    "def plot_results():        \n",
    "    # Plot a projection of test prediction on the first two predictors\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_test[:,1], X_test[:,2], Y_test, s=1, c=\"#000000\")\n",
    "    ax.scatter(X_test[:,1], X_test[:,2], output.reshape([-1,1]), s=1, c=\"#FF0000\")\n",
    "    plt.xlabel('X_1')\n",
    "    plt.ylabel('X_2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "n_points = 5000\n",
    "n_features = 3\n",
    "learning_rate = 0.05\n",
    "n_steps = 1000\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = gen_tobit_dataset(n_points, n_features)\n",
    "reset_graph()\n",
    "weights, loss, std_model, error_L1, output = train_model(n_features, learning_rate, n_steps)\n",
    "\n",
    "part_1=list(weights.squeeze())\n",
    "try:\n",
    "    part1 = \" \".join(map(repr, part_1))\n",
    "except TypeError:\n",
    "    part1 = repr(part_1)\n",
    "submissions[all_parts[0]]=part1\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[0],all_parts,submissions)\n",
    "weights.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part_2=[loss, std_model, error_L1]\n",
    "try:\n",
    "    part2 = \" \".join(map(repr, part_2))\n",
    "except TypeError:\n",
    "    part2 = repr(part_2)  \n",
    "    \n",
    "submissions[all_parts[1]]=part2\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:2],all_parts,submissions)\n",
    "[loss, std_model, error_L1]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Linear regression and Neural Network to Non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(n_points=10000, n_features=3, use_nonlinear=True, \n",
    "                    noise_std=0.1, train_test_split = 4):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_points - number of data points to generate\n",
    "    n_features - a positive integer - number of features\n",
    "    use_nonlinear - if True, generate non-linear data\n",
    "    train_test_split - an integer - what portion of data to use for testing\n",
    "    \n",
    "    Return:\n",
    "    X_train, Y_train, X_test, Y_test, n_train, n_features\n",
    "    \"\"\"\n",
    "    # Linear data or non-linear data?\n",
    "    if use_nonlinear:\n",
    "        weights = np.array([[1.0, 0.5, 0.2],[0.5, 0.3, 0.15]], dtype=np.float32)\n",
    "    else:\n",
    "        weights = np.array([1.0, 0.5, 0.2], dtype=np.float32)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    bias = np.ones(n_points).reshape((-1,1))\n",
    "    low = - np.ones((n_points,n_features), dtype=np.float32)\n",
    "    high = np.ones((n_points,n_features), dtype=np.float32)\n",
    "    \n",
    "    X = np.random.uniform(low=low, high=high)\n",
    "    noise = np.random.normal(size=(n_points, 1))\n",
    "    noise_std = 0.1\n",
    "    \n",
    "    if use_nonlinear:\n",
    "        Y = (weights[0,0] * bias + np.dot(X, weights[0, :]).reshape((-1,1)) + \n",
    "             np.dot(X*X, weights[1, :]).reshape([-1,1]) +\n",
    "             noise_std * noise)\n",
    "    else:\n",
    "        Y = (weights[0] * bias + np.dot(X, weights[:]).reshape((-1,1)) + \n",
    "             noise_std * noise)\n",
    "    \n",
    "    n_test = int(n_points/train_test_split)\n",
    "    n_train = n_points - n_test\n",
    "    \n",
    "    X_train = X[:n_train,:]\n",
    "    Y_train = Y[:n_train].reshape((-1,1))\n",
    "\n",
    "    X_test = X[n_train:,:]\n",
    "    Y_test = Y[n_train:].reshape((-1,1))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, n_train, n_features\n",
    "\n",
    "X_train, Y_train, X_test, Y_test, n_train, n_features = generate_data(use_nonlinear=False)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train, Y_train, X_test, Y_test, n_train, n_features = generate_data(use_nonlinear=True)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "Implement sklearn_lin_regress() function which returns a tuple of\n",
    "\n",
    "- coefficients of linear regression\n",
    "- an instance of LinearRegression class trained to X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sklearn_lin_regress\n",
    "def sklearn_lin_regress(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_train  - np.array of size (n by k) where n is number of observations \n",
    "                of independent variables and k is number of variables\n",
    "    Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    Return: a tuple of \n",
    "      - np.array of size (k+1 by 1) of regression coefficients\n",
    "      - an instance of LinearRegression\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr_model = None\n",
    "    theta_sklearn = np.array([], dtype=np.float32)\n",
    "    ### START CODE HERE ### (≈ 2-3 lines of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return theta_sklearn, lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "### GRADED PART (DO NOT EDIT) ###\n",
    "theta_sklearn, lr_model = sklearn_lin_regress(X_train, Y_train)\n",
    "\n",
    "part_3 = list(theta_sklearn.squeeze())\n",
    "try:\n",
    "    part3 = \" \".join(map(repr, part_3))\n",
    "except TypeError:\n",
    "    part3 = repr(part_3)\n",
    "    \n",
    "submissions[all_parts[2]]=part3\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:3],all_parts,submissions)\n",
    "\n",
    "theta_sklearn.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression.score() computes $R^2$ coefficient. The coefficient $R^2$ is defined as $(1 - \\frac{u}{v})$, where u is the residual sum of squares $\\sum (y\\_true - y\\_pred)^2$ and v is the total sum of squares $\\sum (y\\_true - \\bar{y\\_true})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "### GRADED PART (DO NOT EDIT) ###\n",
    "# calculate Linear Regression score\n",
    "model_score = 0.\n",
    "if lr_model is not None:\n",
    "    model_score = lr_model.score(X_test, Y_test)\n",
    "part4=str(model_score)\n",
    "submissions[all_parts[3]]=part4\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:4],all_parts,submissions)\n",
    "model_score\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with Tensorflow \n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Construct two-layer Neural Network utilizing neuron_layer() function. The number of nodes in two hidden layers are defined by n_hidden1 and n_hidden2, respectively. Use Gradient Descent Optimizer.\n",
    "\n",
    "The train the network using X_train / y_train and compute accuracy of the prediction using X_test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    np.random.seed(42)\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch\n",
    "    \n",
    "def neuron_layer(X, n_neurons, name, activation_fn=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation_fn is not None:\n",
    "            return activation_fn(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden1 = 100\n",
    "n_hidden2 = 120\n",
    "n_outputs = 1 # single value prediction\n",
    "n_inputs = X_test.shape[1]\n",
    "\n",
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "\n",
    "### START CODE HERE ### (≈ 10-15 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_epochs = 200\n",
    "batch_size = 60\n",
    "num_rec = X_train.shape[0]\n",
    "n_batches = int(np.ceil(num_rec / batch_size))\n",
    "acc_test = 0. #  assign the result of accuracy testing to this variable\n",
    "\n",
    "### START CODE HERE ### (≈ 9-10 lines of code)\n",
    "with tf.Session() as sess:\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "part5=str(acc_test)\n",
    "submissions[all_parts[4]]=part5\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key, all_parts[:5],all_parts,submissions)\n",
    "acc_test\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "guided-tour-machine-learning-finance"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
