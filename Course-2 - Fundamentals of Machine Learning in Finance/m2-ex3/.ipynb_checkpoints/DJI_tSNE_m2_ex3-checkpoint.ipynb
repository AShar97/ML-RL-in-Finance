{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization with t-SNE\n",
    "\n",
    "Welcome to your 3-rd assignment in Unsupervised Machine Learning in Finance. This exercise will provide hands-on experience with non-linear models such as KernelPCA and t-SNE.\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
    "- Do not modify the (# GRADED FUNCTION [function name]) comment in some cells. Your work would not be graded if you change this. Each cell containing that comment should only contain one function.\n",
    "- After coding your function, run the cell right below it to check if your result is correct.\n",
    "\n",
    "**After this assignment you will:**\n",
    "- Be able to use KernelPCA to construct eigen-portfolios\n",
    "- Calculate un-expected log-returns \n",
    "- Visualize multi-dimensional data using t-SNE\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook. \n",
    "\n",
    "We will often specify \"(≈ X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import operator\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ONLY FOR GRADING. DO NOT EDIT ###\n",
    "submissions=dict()\n",
    "assignment_key=\"SgjoDxBsEeidDQqwEEcflg\" \n",
    "all_parts=[\"yzL4C\", \"B3CHT\", \"jxlkt\",\"miiAE\",\"VOnND\"]\n",
    "### ONLY FOR GRADING. DO NOT EDIT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COURSERA_TOKEN = # the key provided to the Student under his/her email on submission page\n",
    "# COURSERA_EMAIL = # the email\n",
    "COURSERA_TOKEN=\" \"\n",
    "COURSERA_EMAIL=\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_for_nulls(df):\n",
    "    \"\"\"\n",
    "    Test and report number of NAs in each column of the input data frame\n",
    "    :param df: pandas.DataFrame\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for col in df.columns.values:\n",
    "        num_nans = np.sum(df[col].isnull())\n",
    "        if num_nans > 0:\n",
    "            print('%d Nans in col %s' % (num_nans, col))\n",
    "    print('New shape of df: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "asset_prices = pd.read_csv('/home/jovyan/work/readonly/spx_holdings_and_spx_closeprice_m2-ex3.csv',\n",
    "                     date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
    "                     index_col = 0).dropna()\n",
    "n_stocks_show = 12\n",
    "print('Asset prices shape', asset_prices.shape)\n",
    "asset_prices.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Last column contains SPX index prices:')\n",
    "asset_prices.iloc[:, -10:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_nulls(asset_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate price log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_returns = np.log(asset_prices) - np.log(asset_prices.shift(1))\n",
    "asset_returns = asset_returns.iloc[1:, :]\n",
    "asset_returns.iloc[:, :n_stocks_show].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 (Calculate Moving Average)\n",
    "**Instructions:**\n",
    "\n",
    "- Calculate 20 and 100-day moving average of SPX Index price based on **spx_index** pd.core.series.Series\n",
    "- Assign results to **short_rolling_spx** and **long_rolling_spx** respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the SPX time series. This now returns a Pandas Series object indexed by date.# Get t \n",
    "spx_index = asset_prices.loc[:, 'SPX']\n",
    "\n",
    "short_rolling_spx = pd.core.series.Series(np.zeros(len(asset_prices.index)), index=asset_prices.index)\n",
    "long_rolling_spx = short_rolling_spx\n",
    "\n",
    "# Calculate the 20 and 100 days moving averages of log-returns\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "### ...\n",
    "\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the index and rolling averages\n",
    "fig=plt.figure(figsize=(12, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(spx_index.index, spx_index, label='SPX Index')\n",
    "ax.plot(short_rolling_spx.index, short_rolling_spx, label='20 days rolling')\n",
    "ax.plot(long_rolling_spx.index, long_rolling_spx, label='100 days rolling')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Log returns')\n",
    "ax.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "np.random.seed(42)\n",
    "idx_test = np.random.randint(low=100, high=len(short_rolling_spx), size=50)\n",
    "result = short_rolling_spx.values[idx_test] + long_rolling_spx.values[idx_test] \n",
    "\n",
    "\n",
    "### grading results ###\n",
    "part_1 = list(result.squeeze())\n",
    "try:\n",
    "    part1 = \" \".join(map(repr, part_1))\n",
    "except TypeError:\n",
    "    part1 = repr(part_1)\n",
    "submissions[all_parts[0]]=part1\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,all_parts[:1],all_parts,submissions)\n",
    "result.squeeze()\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply scikit-learn StandardScaler to stocks log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "# Centering and scaling happen independently on each feature by computing the relevant statistics \n",
    "# on the samples in the training set. Mean and standard deviation are then stored to be used on later \n",
    "# data using the transform method.\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "log_ret_mat_std = StandardScaler().fit_transform(asset_returns.values)\n",
    "log_ret_df_std = pd.DataFrame(data=log_ret_mat_std, \n",
    "                              index=asset_returns.index,\n",
    "                              columns=asset_returns.columns.values) \n",
    "log_ret_df_std.iloc[:, :10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 20 and 100 days moving averages of the log-returns\n",
    "short_rolling_spx = log_ret_df_std[['SPX']].rolling(window=20).mean()\n",
    "long_rolling_spx = log_ret_df_std[['SPX']].rolling(window=100).mean()\n",
    "\n",
    "# Plot the index and rolling averages\n",
    "fig=plt.figure(figsize=(12, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(log_ret_df_std.index, log_ret_df_std[['SPX']], label='SPX Index')\n",
    "ax.plot(short_rolling_spx.index, short_rolling_spx, label='20 days rolling')\n",
    "ax.plot(long_rolling_spx.index, long_rolling_spx, label='100 days rolling')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Log returns')\n",
    "ax.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a label 'regime' to each date:\n",
    "# 'regime' = 'benign' for all points except two intervals\n",
    "# 'regime' = 'crisis_2001_2002', or\n",
    "# 'regime = ', 'crisis_2007-2009'\n",
    "\n",
    "# first assign the default value for all rows\n",
    "log_ret_df_std['regime'] = 'benign'\n",
    "dt_start = np.datetime64('2000-03-24T00:00:00.000000000')\n",
    "dt_end = np.datetime64('2002-10-09T00:00:00.000000000')\n",
    "flag_crisis_2001_2002 = np.logical_and(log_ret_df_std.index > dt_start, log_ret_df_std.index < dt_end)\n",
    "\n",
    "dt_start = np.datetime64('2007-10-09T00:00:00.000000000')\n",
    "dt_end = np.datetime64('2009-03-09T00:00:00.000000000')\n",
    "flag_crisis_2007_2009 = np.logical_and(log_ret_df_std.index > dt_start, log_ret_df_std.index < dt_end)\n",
    "\n",
    "log_ret_df_std.loc[flag_crisis_2001_2002,'regime'] = 'crisis_2001_2002'\n",
    "log_ret_df_std.loc[flag_crisis_2007_2009, 'regime'] = 'crisis_2007_2009'\n",
    "\n",
    "print('crisis_2001_2002', log_ret_df_std[log_ret_df_std.regime == 'crisis_2001_2002'].shape[0])\n",
    "print('crisis_2007_2009', log_ret_df_std[log_ret_df_std.regime == 'crisis_2007_2009'].shape[0])\n",
    "print(log_ret_df_std.shape)\n",
    "\n",
    "print('Last N days of the dataset:')\n",
    "log_ret_df_std.iloc[:, :10].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data before 2012-03-26 for training, and data after it for testing\n",
    "\n",
    "train_end = datetime.datetime(2012, 3, 26) \n",
    "df_train = log_ret_df_std[log_ret_df_std.index <= train_end].copy()\n",
    "df_test = log_ret_df_std[log_ret_df_std.index > train_end].copy()\n",
    "print('Train dataset:', df_train.shape)\n",
    "print('Test dataset:', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 (Returns regression on SPX Index)\n",
    "**Instructions:**\n",
    "\n",
    "- Compute $R^2$, $\\alpha$, and $\\beta$ for in-sample and out-of-sample regressing each stock returns on SPX returns. Use df_train and df_test data. \n",
    "- Store  in-sample $R^2$ in **R2_in_sample** list\n",
    "- Store  out-of-sample $R^2$ in **R2_out_sample** list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regress each individual stock on the market\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a Linear Regression object\n",
    "lm = LinearRegression()\n",
    "stock_tickers = asset_returns.columns.values[:-1] # exclude SPX\n",
    "\n",
    "# compute betas for all stocks in the dataset\n",
    "R2_in_sample = [0.] * len(stock_tickers)\n",
    "R2_out_sample = [0.] * len(stock_tickers)\n",
    "betas = [0.] * len(stock_tickers)\n",
    "alphas = [0.] * len(stock_tickers)\n",
    "\n",
    "### START CODE HERE ### (≈ 10-12 lines of code)\n",
    "### ...\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = pd.DataFrame({'R2 in-sample': R2_in_sample, 'R2 out-sample': R2_out_sample, 'Alpha': alphas, 'Beta': betas}, \n",
    "                     index=stock_tickers)\n",
    "df_lr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "\n",
    "np.random.seed(42)\n",
    "idx = np.random.randint(low=0, high=df_lr.shape[0], size=50)\n",
    "### grading results ###\n",
    "part_5 = list(df_lr.as_matrix()[idx, :].flatten())\n",
    "try:\n",
    "    part5 = \" \".join(map(repr, part_5))\n",
    "except TypeError:\n",
    "    part5 = repr(part_5)\n",
    "submissions[all_parts[4]]=part5\n",
    "set_parts=[all_parts[0], all_parts[4]]\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,set_parts,all_parts,submissions)\n",
    "\n",
    "\n",
    "df_lr.as_matrix()[idx, :].flatten()\n",
    "\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 (Calculation of unexpected log-returns)\n",
    "**Instructions:**\n",
    "- Use **df_train**  and calculated in Part 2 **df_lr** with $\\beta$ and $\\alpha$ to compute unexpected log returns\n",
    "- Calculate unexplained log-returns as difference between the stock return and its value, \"predicted\" by the index return.\n",
    "\n",
    "$$ \\epsilon^i_t = R^i_t - \\alpha_i - \\beta_i R^M_t$$\n",
    "- Store unexplained log-returns in df_unexplained pnadas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unexplained = df_train.loc[:, stock_tickers]\n",
    "\n",
    "### START CODE HERE ### (≈ 4-10 lines of code)\n",
    "### ...\n",
    "\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print('Unexplained log-returns of S&P 500 Index stocks', df_unexplained.shape)\n",
    "print('Unexplained log-returns of S&P 500 Index stocks:')\n",
    "df_unexplained.iloc[:, :10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "np.random.seed(42)\n",
    "idx_row = np.random.randint(low=0, high=df_lr.shape[0], size=100)\n",
    "np.random.seed(42)\n",
    "idx_col = np.random.randint(low=0, high=df_lr.shape[1], size=100)\n",
    "\n",
    "# grading\n",
    "part_2=list(df_unexplained.as_matrix()[idx_row, idx_col])\n",
    "try:\n",
    "    part2 = \" \".join(map(repr, part_2))\n",
    "except TypeError:\n",
    "    part2 = repr(part_2)\n",
    "submissions[all_parts[1]]=part2\n",
    "set_parts=[all_parts[0], all_parts[1], all_parts[4]]\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,set_parts,all_parts,submissions)\n",
    "\n",
    "df_unexplained.as_matrix()[idx_row, idx_col]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 (Kernel PCA of Covariance Matrix of Returns)\n",
    "\n",
    "**Instructions:**\n",
    "- Perform Kernel PCA with 1 component using returns data **df_test** for all stocks in df_test\n",
    "- Transform original mapping in the coordinates of the first principal component\n",
    "- Assign tranformed returns to PCA_1 in **** DataFrame\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df_train.loc[:, ['SPX', 'GE', 'AAPL', 'MSFT', 'regime']], \n",
    "             vars=['SPX', 'GE', 'AAPL', 'MSFT'], hue=\"regime\", size=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_tickers = asset_returns.columns.values[:-1]\n",
    "assert 'SPX' not in stock_tickers, \"By accident included SPX index\"\n",
    "data = df_test[stock_tickers].values\n",
    "\n",
    "df_index_test = pd.DataFrame(data=df_test['SPX'].values, index=df_test.index, columns=['SPX'])\n",
    "df_index_test['PCA_1'] = np.ones(len(df_test.index)) \n",
    "\n",
    "### START CODE HERE ### (≈ 2-3 lines of code)\n",
    "# please set random_state=42 when initializing Kernel PCA\n",
    "\n",
    "\n",
    "### GRADED PART (DO NOT EDIT) ###\n",
    "\n",
    "# draw the two plots\n",
    "df_plot = df_index_test[['SPX', 'PCA_1']].apply(lambda x: (x - x.mean()) / x.std())\n",
    "df_plot.plot(figsize=(12, 6), title='Index replication via PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "np.random.seed(42)\n",
    "transformed_first_pc = df_index_test['PCA_1'].values\n",
    "idx_test = np.random.randint(low=0, high=len(transformed_first_pc), size=100)\n",
    "\n",
    "#grading\n",
    "part_3=list(np.absolute(transformed_first_pc[idx_test]))\n",
    "try:\n",
    "    part3 = \" \".join(map(repr, part_3))\n",
    "except TypeError:\n",
    "    part3 = repr(part_3)\n",
    "submissions[all_parts[2]]=part3\n",
    "set_parts=[all_parts[0], all_parts[1], all_parts[2], all_parts[4]]\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,set_parts,all_parts,submissions)\n",
    "\n",
    "np.absolute(transformed_first_pc[idx_test]) # because PCA results match down to a sign\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 (Visualization with t-SNE)\n",
    "\n",
    "Lets turn attention to a popular dimensonality reduction algorithm: t-distributed stochastic neighbor embedding (t-SNE). Developed by Laurens van der Maaten and Geoffrey Hinton (see the original paper here), this algorithm has been successfully applied to many real-world datasets. \n",
    "\n",
    "The t-SNE algorithm provides an effective method to visualize a complex dataset. It successfully uncovers hidden structures in the data, exposing natural clusters and smooth nonlinear variations along the dimensions. It has been implemented in many languages, including Python, and it can be easily used thanks to the scikit-learn library.\n",
    "\n",
    "**Instructions:**\n",
    "- Fit TSNE with 2 components, 300 iterations. Set perplexity to 50.\n",
    "- Use **log_ret_df_std** dataset for stock tickers only\n",
    "- Store the results of fitting in **tsne_results** np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(42)\n",
    "tsne_results = np.zeros((log_ret_df_std[stock_tickers].shape[0], 2))\n",
    "perplexity = 50 \n",
    "n_iter = 300\n",
    "time_start = time.time()\n",
    "### START CODE HERE ### (≈ 2-3 lines of code)\n",
    "#... please set random_state=42 when initializing TSNE\n",
    "\n",
    "\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame({'regime': log_ret_df_std.regime.values,\n",
    "                        'x-tsne': tsne_results[:,0],\n",
    "                        'y-tsne': tsne_results[:,1]},\n",
    "                       index=log_ret_df_std.index)\n",
    "print('t-SNE (perplexity=%.0f) data:' % perplexity)\n",
    "df_tsne.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED PART (DO NOT EDIT) ###\n",
    "np.random.seed(42)\n",
    "idx_row = np.random.randint(low=0, high=tsne_results.shape[0], size=100)\n",
    "np.random.seed(42)\n",
    "idx_col = np.random.randint(low=0, high=tsne_results.shape[1], size=100)\n",
    "\n",
    "#grading\n",
    "part_4 = list(tsne_results[idx_row, idx_col]) # because PCA results match down to a sign\n",
    "try:\n",
    "    part4 = \" \".join(map(repr, part_4))\n",
    "except TypeError:\n",
    "    part4 = repr(part_4)\n",
    "submissions[all_parts[3]]=part4\n",
    "set_parts=[all_parts[0], all_parts[1], all_parts[2], all_parts[3], all_parts[4]]\n",
    "grading.submit(COURSERA_EMAIL, COURSERA_TOKEN, assignment_key,set_parts,all_parts,submissions)\n",
    "\n",
    "tsne_results[idx_row, idx_col]\n",
    "### GRADED PART (DO NOT EDIT) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tsne_2D(df_tsne, label_column, plot_title):\n",
    "    \"\"\"\n",
    "    plot_tsne_2D - plots t-SNE as two-dimensional graph\n",
    "    Arguments:\n",
    "    label_column - column name where labels data is stored\n",
    "    df_tsne - pandas.DataFrame with columns x-tsne, y-tsne\n",
    "    plot_title - string\n",
    "    \"\"\"\n",
    "    unique_labels = df_tsne[label_column].unique()\n",
    "    print('Data labels:', unique_labels)\n",
    "    print(df_tsne.shape)\n",
    "\n",
    "    colors = [ 'b', 'g','r']\n",
    "    markers = ['s', 'x', 'o']\n",
    "    y_train = df_tsne.regime.values\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ix = 0\n",
    "    bars = [None] * len(unique_labels)\n",
    "    for label, c, m in zip(unique_labels, colors, markers):\n",
    "        plt.scatter(df_tsne.loc[df_tsne[label_column]==label, 'x-tsne'], \n",
    "                    df_tsne.loc[df_tsne[label_column]==label, 'y-tsne'], \n",
    "                    c=c, label=label, marker=m, s=15)\n",
    "        bars[ix] = plt.bar([0, 1, 2], [0.2, 0.3, 0.1], width=0.4, align=\"center\", color=c)\n",
    "        ix += 1\n",
    "\n",
    "    plt.legend(bars, unique_labels)\n",
    "    plt.xlabel('first dimension')\n",
    "    plt.ylabel('second dimension')\n",
    "    plt.title(plot_title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_2D(df_tsne, 'regime', 'S&P 500 dimensionality reduction with t-SNE (perplexity=%d)' % perplexity)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "machine-learning-in-finance",
   "graded_item_id": "gW5TE",
   "launcher_item_id": "NPbti"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
